{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with Zstandard-compressed Datasets from Hugging Face\n",
    "\n",
    "This notebook demonstrates how to work with zstd-compressed files from Hugging Face datasets, focusing on the \"agentlans/bluesky\" dataset that uses Zstandard (zstd) compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Working with zstd-compressed Hugging Face Datasets\n",
    "This notebook demonstrates how to work with zstd-compressed datasets from Hugging Face, specifically for the \"agentlans/bluesky\" dataset.\n",
    "\n",
    "The standard `datasets.load_dataset()` method doesn't work well with zstd-compressed files due to compatibility issues. Instead, we'll use a direct approach:\n",
    "1. Download files using `huggingface_hub`\n",
    "2. Manually decompress with the `zstandard` library\n",
    "3. Parse the JSON data\n",
    "\n",
    "Let's start by installing the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zstandard as zstd\n",
    "import json\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "# Download a specific language file (smaller file for demonstration)\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=\"agentlans/bluesky\",\n",
    "    filename=\"en.jsonl.zst\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "print(f\"Downloaded file to: {file_path}\")\n",
    "\n",
    "# Decompress and read the file\n",
    "with open(file_path, 'rb') as f:\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    with dctx.stream_reader(f) as reader:\n",
    "        # Read the entire file (since it's small)\n",
    "        data = reader.read().decode('utf-8')\n",
    "        \n",
    "        # Parse JSON lines\n",
    "        lines = data.strip().split('\\n')\n",
    "        \n",
    "        # Show the first few samples\n",
    "        for i, line in enumerate(lines[:5]):\n",
    "            sample = json.loads(line)\n",
    "            print(f\"\\nSample {i+1}:\")\n",
    "            print(f\"Text: {sample['text']}\")\n",
    "            print(f\"Language: {sample['language']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Working with Small Files\n",
    "\n",
    "First, let's look at how to work with smaller language-specific files:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zstandard as zstd\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "import io\n",
    "\n",
    "# Download the large file\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=\"agentlans/bluesky\",\n",
    "    filename=\"all.jsonl.zst\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "print(f\"Downloaded large file to: {file_path}\")\n",
    "\n",
    "def process_chunks(file_path, chunk_size=1024*1024):\n",
    "    \"\"\"Process a large zstd-compressed JSONL file in chunks.\"\"\"\n",
    "    samples_processed = 0\n",
    "    samples_to_show = 5\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        with dctx.stream_reader(f) as reader:\n",
    "            text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "            \n",
    "            # Process line by line\n",
    "            buffer = \"\"\n",
    "            for i, line in enumerate(text_stream):\n",
    "                if samples_processed < samples_to_show:\n",
    "                    try:\n",
    "                        sample = json.loads(line)\n",
    "                        print(f\"\\nSample {samples_processed+1} from large file:\")\n",
    "                        print(f\"Text: {sample['text'][:100]}...\" if len(sample['text']) > 100 else f\"Text: {sample['text']}\")\n",
    "                        print(f\"Language: {sample['language']}\")\n",
    "                        samples_processed += 1\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Error decoding JSON on line {i}\")\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"\\nProcessed {samples_processed} samples from the large file\")\n",
    "            print(f\"Note: The full file contains many more samples\")\n",
    "\n",
    "# Process the large file\n",
    "process_chunks(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a Dataset Iterator\n",
    "\n",
    "For more advanced processing, we can create an iterator that yields samples one at a time:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zstandard as zstd\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "import io\n",
    "from typing import Iterator, Dict, Any\n",
    "\n",
    "def bluesky_dataset_iterator(repo_id: str, filename: str) -> Iterator[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create an iterator for a zstd-compressed JSONL dataset from Hugging Face.\n",
    "    \n",
    "    Args:\n",
    "        repo_id: The Hugging Face dataset repository ID\n",
    "        filename: The specific file to download from the repository\n",
    "        \n",
    "    Returns:\n",
    "        An iterator that yields one sample at a time\n",
    "    \"\"\"\n",
    "    # Download the file\n",
    "    file_path = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=filename,\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    \n",
    "    # Open and process the file\n",
    "    f = open(file_path, 'rb')\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    stream_reader = dctx.stream_reader(f)\n",
    "    text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8')\n",
    "    \n",
    "    # Yield samples one by one\n",
    "    for line in text_stream:\n",
    "        try:\n",
    "            sample = json.loads(line)\n",
    "            yield sample\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing line: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Clean up\n",
    "    text_stream.close()\n",
    "    stream_reader.close()\n",
    "    f.close()\n",
    "\n",
    "# Demonstrate the iterator\n",
    "dataset = bluesky_dataset_iterator(\"agentlans/bluesky\", \"en.jsonl.zst\")\n",
    "\n",
    "# Print the first 5 samples\n",
    "for i, sample in enumerate(dataset):\n",
    "    if i < 5:\n",
    "        print(f\"\\nSample {i+1} (using iterator):\")\n",
    "        print(f\"Text: {sample['text'][:100]}...\" if len(sample['text']) > 100 else f\"Text: {sample['text']}\")\n",
    "        print(f\"Language: {sample['language']}\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparison with Standard Dataset Loading\n",
    "\n",
    "For comparison, here's what happens when we try the standard `datasets.load_dataset()` approach. This will likely fail due to the zstd compatibility issues:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    # Attempt to load the dataset using the standard approach\n",
    "    print(\"Attempting to load dataset using datasets.load_dataset()...\")\n",
    "    dataset = load_dataset(\"agentlans/bluesky\", split=\"train\")\n",
    "    \n",
    "    # If it succeeds, show a sample\n",
    "    print(\"Success! Showing first sample:\")\n",
    "    print(dataset[0])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset with datasets.load_dataset(): {e}\")\n",
    "    print(\"\\nThis is why we need the manual approach demonstrated in this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated how to work with zstd-compressed datasets from Hugging Face:\n",
    "\n",
    "1. **Direct Approach for Small Files**:\n",
    "   - Download specific files using `huggingface_hub.hf_hub_download()`\n",
    "   - Decompress with zstandard library\n",
    "   - Parse and process the data\n",
    "\n",
    "2. **Streaming Approach for Large Files**:\n",
    "   - Process large files in chunks to avoid memory issues\n",
    "   - Use streaming IO with zstandard\n",
    "   - Handle each line individually\n",
    "\n",
    "3. **Dataset Iterator**:\n",
    "   - Create a reusable iterator function\n",
    "   - Process samples one at a time\n",
    "   - Properly handle resources with cleanup\n",
    "\n",
    "This approach works reliably even when the standard `datasets.load_dataset()` method fails due to compatibility issues with zstd compression."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install zstandard huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zstandard in /home/vscode/.local/lib/python3.10/site-packages (0.23.0)\r\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.30.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2025.3.2)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (25.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.18.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.13.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2025.4.26)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\r\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install zstandard huggingface_hub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
