{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": "# Working with Zstandard-compressed Datasets from Hugging Face\n\nThis notebook demonstrates how to work with zstd-compressed files from Hugging Face datasets, focusing on the \"agentlans/bluesky\" dataset that uses Zstandard (zstd) compression."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": "## Working with zstd-compressed Hugging Face Datasets\nThis notebook demonstrates how to work with zstd-compressed datasets from Hugging Face, specifically for the \"agentlans/bluesky\" dataset.\n\nThe standard `datasets.load_dataset()` method doesn't work well with zstd-compressed files due to compatibility issues. Instead, we'll use a direct approach:\n1. Download files using `huggingface_hub`\n2. Manually decompress with the `zstandard` library\n3. Parse the JSON data\n\nLet's start by installing the required packages:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import zstandard as zstd\nimport json\nfrom huggingface_hub import hf_hub_download\nimport os\n\n# Download a specific language file (smaller file for demonstration)\nfile_path = hf_hub_download(\n    repo_id=\"agentlans/bluesky\",\n    filename=\"en.jsonl.zst\",\n    repo_type=\"dataset\"\n)\n\nprint(f\"Downloaded file to: {file_path}\")\n\n# Decompress and read the file\nwith open(file_path, 'rb') as f:\n    dctx = zstd.ZstdDecompressor()\n    with dctx.stream_reader(f) as reader:\n        # Read the entire file (since it's small)\n        data = reader.read().decode('utf-8')\n        \n        # Parse JSON lines\n        lines = data.strip().split('\\n')\n        \n        # Show the first few samples\n        for i, line in enumerate(lines[:5]):\n            sample = json.loads(line)\n            print(f\"\\nSample {i+1}:\")\n            print(f\"Text: {sample['text']}\")\n            print(f\"Language: {sample['language']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": "## Working with Small Files\n\nFirst, let's look at how to work with smaller language-specific files:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import zstandard as zstd\nfrom huggingface_hub import hf_hub_download\nimport json\nimport io\n\n# Download the large file\nfile_path = hf_hub_download(\n    repo_id=\"agentlans/bluesky\",\n    filename=\"all.jsonl.zst\",\n    repo_type=\"dataset\"\n)\n\nprint(f\"Downloaded large file to: {file_path}\")\n\ndef process_chunks(file_path, chunk_size=1024*1024):\n    \"\"\"Process a large zstd-compressed JSONL file in chunks.\"\"\"\n    samples_processed = 0\n    samples_to_show = 5\n    \n    with open(file_path, 'rb') as f:\n        dctx = zstd.ZstdDecompressor()\n        with dctx.stream_reader(f) as reader:\n            text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n            \n            # Process line by line\n            buffer = \"\"\n            for i, line in enumerate(text_stream):\n                if samples_processed < samples_to_show:\n                    try:\n                        sample = json.loads(line)\n                        print(f\"\\nSample {samples_processed+1} from large file:\")\n                        print(f\"Text: {sample['text'][:100]}...\" if len(sample['text']) > 100 else f\"Text: {sample['text']}\")\n                        print(f\"Language: {sample['language']}\")\n                        samples_processed += 1\n                    except json.JSONDecodeError:\n                        print(f\"Error decoding JSON on line {i}\")\n                else:\n                    break\n            \n            # Print summary\n            print(f\"\\nProcessed {samples_processed} samples from the large file\")\n            print(f\"Note: The full file contains many more samples\")\n\n# Process the large file\nprocess_chunks(file_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": "## Creating a Dataset Iterator\n\nFor more advanced processing, we can create an iterator that yields samples one at a time:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import zstandard as zstd\nfrom huggingface_hub import hf_hub_download\nimport json\nimport io\nfrom typing import Iterator, Dict, Any\n\ndef bluesky_dataset_iterator(repo_id: str, filename: str) -> Iterator[Dict[str, Any]]:\n    \"\"\"\n    Create an iterator for a zstd-compressed JSONL dataset from Hugging Face.\n    \n    Args:\n        repo_id: The Hugging Face dataset repository ID\n        filename: The specific file to download from the repository\n        \n    Returns:\n        An iterator that yields one sample at a time\n    \"\"\"\n    # Download the file\n    file_path = hf_hub_download(\n        repo_id=repo_id,\n        filename=filename,\n        repo_type=\"dataset\"\n    )\n    \n    # Open and process the file\n    f = open(file_path, 'rb')\n    dctx = zstd.ZstdDecompressor()\n    stream_reader = dctx.stream_reader(f)\n    text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8')\n    \n    # Yield samples one by one\n    for line in text_stream:\n        try:\n            sample = json.loads(line)\n            yield sample\n        except json.JSONDecodeError:\n            continue\n        except Exception as e:\n            print(f\"Error processing line: {e}\")\n            continue\n    \n    # Clean up\n    text_stream.close()\n    stream_reader.close()\n    f.close()\n\n# Demonstrate the iterator\ndataset = bluesky_dataset_iterator(\"agentlans/bluesky\", \"en.jsonl.zst\")\n\n# Print the first 5 samples\nfor i, sample in enumerate(dataset):\n    if i < 5:\n        print(f\"\\nSample {i+1} (using iterator):\")\n        print(f\"Text: {sample['text'][:100]}...\" if len(sample['text']) > 100 else f\"Text: {sample['text']}\")\n        print(f\"Language: {sample['language']}\")\n    else:\n        break"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": "## Comparison with Standard Dataset Loading\n\nFor comparison, here's what happens when we try the standard `datasets.load_dataset()` approach. This will likely fail due to the zstd compatibility issues:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "try:\n    from datasets import load_dataset\n    \n    # Attempt to load the dataset using the standard approach\n    print(\"Attempting to load dataset using datasets.load_dataset()...\")\n    dataset = load_dataset(\"agentlans/bluesky\", split=\"train\")\n    \n    # If it succeeds, show a sample\n    print(\"Success! Showing first sample:\")\n    print(dataset[0])\n    \nexcept Exception as e:\n    print(f\"Error loading dataset with datasets.load_dataset(): {e}\")\n    print(\"\\nThis is why we need the manual approach demonstrated in this notebook.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": "## Summary\n\nIn this notebook, we've demonstrated how to work with zstd-compressed datasets from Hugging Face:\n\n1. **Direct Approach for Small Files**:\n   - Download specific files using `huggingface_hub.hf_hub_download()`\n   - Decompress with zstandard library\n   - Parse and process the data\n\n2. **Streaming Approach for Large Files**:\n   - Process large files in chunks to avoid memory issues\n   - Use streaming IO with zstandard\n   - Handle each line individually\n\n3. **Dataset Iterator**:\n   - Create a reusable iterator function\n   - Process samples one at a time\n   - Properly handle resources with cleanup\n\nThis approach works reliably even when the standard `datasets.load_dataset()` method fails due to compatibility issues with zstd compression."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install zstandard huggingface_hub"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}